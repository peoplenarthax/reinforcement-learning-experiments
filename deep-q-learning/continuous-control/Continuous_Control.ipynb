{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "This Notebooks uses Unity's ML-Agents to solve a continuous control task where our agents will follow a green ball, for that we need to rotate our arms in the correct way.\n",
    "\n",
    "### Set up\n",
    "\n",
    "This code uses UnityEnvironment (In this case is a customized version from Udacity), NumPy, and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Reacher_Windows_x86_64/Reacher.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unity environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Environment\n",
    "\n",
    "This simulation contains a 20 agents that will be solving the task but contributing to the same learning experience. We do this using DDPG and soft updates\n",
    "\n",
    "At each time step, it has a continuous action of size 4 (a vector containing 4 values between -1 and 1) which controls the arm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the environment look and random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method\n",
    "\n",
    "In order to solve this environment I decided to use a Acto-Critic algorithm based on [DDPG](https://arxiv.org/pdf/1509.02971.pdf), known as [Distributed Distributional Deep Deterministic Policy Gradient](https://arxiv.org/pdf/1804.08617.pdf) (D4PG for friends).\n",
    "\n",
    "#### Why D4PG?\n",
    "In order to answer this questions I would like to quote the original paper:\n",
    "\n",
    "> In control tasks, commonly seen in the robotics domain, continuous action spaces are the norm.\n",
    "For algorithms such as DQN the policy is only implicitly defined in terms of its value function,\n",
    "with actions selected by maximizing this function. In the continuous control domain this would\n",
    "require either a costly optimization step or discretization of the action space. While discretization is\n",
    "perhaps the most straightforward solution, this can prove a particularly poor approximation in highdimensional settings or those that require finer grained control. Instead, a more principled approach\n",
    "is to parameterize the policy explicitly and directly optimize the long term value of following this\n",
    "policy.\n",
    "\n",
    "Additional to this, we decided to solve an environment which contains 20 agents, hence the original DDPG does not allow for an optimal learning experience for our agents, but instead we use soft updates to update the policy. \n",
    "\n",
    "#### How does DDPG work?\n",
    "\n",
    "Some people will define DDPG as a continuous action space version of DQN, and the similarity is quite undeniable since part of the tricks used are based on the DQN, for example using Target Networks and Replay Buffer. DDPG plays with creating an approximation to Q matrix (actor) and an approximation to the action value function (critic), using both help us reduce the variance of DQN methods as well as get a good performance for continuous action space. There is a very good read about this in [OpenAI learning platform](https://spinningup.openai.com/en/latest/algorithms/ddpg.html) as well as in the [Udacity Deep Reinforcement Learning nanodegree](https://udacity.com).\n",
    "\n",
    "__Tricks__\n",
    "\n",
    "\n",
    "We must remark that DDPG algorithms are off-policy algorithms, that means that the policy that we use to explore is different from the one we are using to exploit, this allow us to use experience replay, which seamlessly distribute the task of gathering experience.\n",
    "\n",
    "Another advantage is that we use target networks, we should not forget that we do not really have a trained agent to which compare our policy, so in order to update our approximations, we are \"basing our guess on another guess\", this is quite inestable as we are continuously updating our network, that is why we use 2 networks per approximation, where one of them will be use as a target and will be updated less times to provide some kind of stability.\n",
    "\n",
    "Last but not least, we have an approximation to Q values and an approximation to the action value function, given the Bellman equation for optimal action-value, we can use the mean-squared Bellman error (MSBE) to understand how good our approximation of Q is based on action-value function. \n",
    "\n",
    "On top of this we run several agents at the same time, all of them use the same networks when acting, and basically we use several agents in parallel to maximize the experience gathering of our network, allowing several agents to explore at the same time and increase the diversity of the samples in there. Then, we will perform a soft update were we use a noise function to update partially values within both approximations. \n",
    "\n",
    "##### Algorithms\n",
    "<img src=\"./DDPG.png\" width=\"500\" />\n",
    "<img src=\"./D4PG.png\" width=\"500\" />\n",
    "\n",
    "#### The networks\n",
    "\n",
    "The architecture of our model is quite simple. For both of them we are using a 3 layer Neural Network with respectively state_size, 256 and 128 units. \n",
    "\n",
    "Then they differ in the output, while the actor will output an action sized (4 in this case) with an activation function `tanh` (our action values are between -1 and 1, hence `tanh` is convenient). On the other side the critic network will output a singular value that correspond to the action-value approximation for the given state. We use `leaky relus` wherever I did not specify the activation function between layers or in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from agent import Agent\n",
    "import torch\n",
    "import time\n",
    "from itertools import count\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "def ddpg(n_episodes=140, max_t=1000):\n",
    "    \"\"\" Deep Deterministic Policy Gradients\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "    \"\"\"\n",
    "    scores_window = deque(maxlen=100)\n",
    "    scores = np.zeros(num_agents)\n",
    "    scores_episode = []\n",
    "    \n",
    "    agents =[] \n",
    "    \n",
    "    for i in range(num_agents):\n",
    "        agents.append(Agent(state_size, action_size, random_seed=0))\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        \n",
    "        for agent in agents:\n",
    "            agent.reset()\n",
    "            \n",
    "        scores = np.zeros(num_agents)\n",
    "            \n",
    "        for t in range(max_t):\n",
    "            actions = np.array([agents[i].act(states[i]) for i in range(num_agents)])\n",
    "            env_info = env.step(actions)[brain_name]        # send the action to the environment\n",
    "            next_states = env_info.vector_observations     # get the next state\n",
    "            rewards = env_info.rewards                     # get the reward\n",
    "            dones = env_info.local_done        \n",
    "            \n",
    "            for i in range(num_agents):\n",
    "                agents[i].step(t,states[i], actions[i], rewards[i], next_states[i], dones[i]) \n",
    " \n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            if np.any(dones):\n",
    "                break \n",
    "        score = np.mean(scores)\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores_episode.append(score)\n",
    "\n",
    "        if np.mean(scores_window)>=30.0:\n",
    "            torch.save(Agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(Agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            \n",
    "    return scores_episode\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./score.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe trained agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 36.52499918360263\n"
     ]
    }
   ],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth', map_location=torch.device('cpu')))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = agent.act(states)                        # select an action (for each agent)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future work\n",
    "\n",
    "There are quite some improvements that I would like to try out in next iterations:\n",
    "\n",
    "- __Correctly apply D4PG__, I believe that my current implementation is a variation on D4PG, I should take some more time to make sure I follow the D4PG algorithm correctly. In this implementation the learning took a lot of time\n",
    "\n",
    "- __Apply Prioritized Experience Replay__, As in DQN, PER has proven to work quite well to increase the speed of learning as we only focus on the more relevant experiences.\n",
    "\n",
    "- __Improve the model__, right now we use a very simplistic Neural Network, if we could add some regularization like Batch Normalization or Dropout layers. Also playing around with the hyperparameters would help us fine tune this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
