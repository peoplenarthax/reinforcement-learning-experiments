{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task\n",
    "\n",
    "You enter the maze in A and at the same time, the minotaur enters in B. The minotaur follows a random walk while staying within the limits of the maze. The minotaurâ€™s walk goes through walls (which obviously you cannot do). At each step, you observe the position of the minotaur, and decide on a one-step move (up, down, right or left) or not to move. If the minotaur catches you, he will eat you. Your objective is to identify a strategy maximizing the probability of exiting the maze (reaching B) before time T.\n",
    "\n",
    "\n",
    "<img src=\"minotaur.PNG\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Formulation\n",
    "\n",
    "Part of this task is to define the Markov Decission Process surrounding this problem.\n",
    "\n",
    "States:\n",
    "- Gridworld of 7x8 = 56\n",
    "- Minotaur can be in either of the 56 cells\n",
    "\n",
    "Total states = 56*56 = 3136\n",
    "\n",
    "Actions:\n",
    "- (0) North, (1)South, (2)East, (3)West\n",
    "\n",
    "Transition probability:\n",
    "- Every action will succesfully move the player to the adjacent cell, unless it hits a wall or a border, in those case the player states in the same cell.\n",
    "\n",
    "Rewards:\n",
    "- Arrive to B: T + 5. End\n",
    "- Hit wall: -5\n",
    "- Movement: -1\n",
    "- Same cell as the minotaur: -20. End\n",
    "- T steps without arriving: -10. End\n",
    "\n",
    "End conditions:\n",
    "- Same cell as the minotaur, Reward: -20\n",
    "- Arrive to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the environment\n",
    "\n",
    "First we need to create the environment with which our agent is going to interact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy, deepcopy\n",
    "import math\n",
    "from collections import deque, defaultdict\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(tuple_a, tuple_b):\n",
    "    return (tuple_a[0] + tuple_b[0], tuple_a[1] + tuple_b[1] )\n",
    "def in_limits(position, limits):\n",
    "    return (position[0] >= 0 and position[0] < limits[0]) and (position[1] >= 0 and position[1] < limits[1])\n",
    "\n",
    "mapGrid = np.array([\n",
    "            [  0, 0, 1, 0, 0,  0, 0, 0],\n",
    "            [  0, 0, 1, 0, 0,  1, 0, 0],\n",
    "            [  0, 0, 1, 0, 0,  1, 1, 1],\n",
    "            [  0, 0, 1, 0, 0,  1, 0, 0],\n",
    "            [  0, 0, 0, 0, 0,  0, 0, 0],\n",
    "            [  0, 1, 1, 1, 1,  1, 1, 0],\n",
    "            [  0, 0, 0, 0, 1,  0, 0, 0]\n",
    "        ])\n",
    "\n",
    "translationDictionary = {\n",
    "            0: (-1, 0),\n",
    "            1: (1, 0),\n",
    "            2: (0, 1),\n",
    "            3: (0, -1)\n",
    "        }\n",
    "\n",
    "def generateMoveDict(grid, ignore_walls=True):\n",
    "    moveDict = [[{0: (x, y), 1:(x, y), 2: (x, y), 3: (x, y)  } \\\n",
    "                 for y in range(grid.shape[1]) ] \\\n",
    "                for x in range(grid.shape[0])]\n",
    "    \n",
    "    for x in range(grid.shape[0]):\n",
    "        for y in range(grid.shape[1]):\n",
    "            moves = moveDict[x][y]\n",
    "            for A in range(4):\n",
    "                next_cell = add(moves[A], translationDictionary[A])\n",
    "                if in_limits(next_cell, grid.shape):\n",
    "                    if ignore_walls:\n",
    "                        moves[A] = next_cell\n",
    "                    else:\n",
    "                        if grid[next_cell[0]][next_cell[1]] != 1:\n",
    "                            moves[A] = next_cell\n",
    "                            \n",
    "            moveDict[x][y] = moves                \n",
    "            \n",
    "    return moveDict\n",
    "\n",
    "def generateStateGrid(grid):\n",
    "    states = [[ deepcopy(grid) for y in range(grid.shape[1]) ] for x in range(grid.shape[0])]\n",
    "    for x in range(grid.shape[0]):\n",
    "        for y in range(grid.shape[1]):\n",
    "            states[x][y][x][y] = 5\n",
    "    return states\n",
    "\n",
    "class MinotaurEnvironment:\n",
    "    def __init__(self, T=20):\n",
    "        self.T = T\n",
    "        self.playerInitialPosition = (0,0)\n",
    "        self.minotaurInitialPosition = (6, 5)\n",
    "        self.playerPos = self.playerInitialPosition\n",
    "        self.minotaurPos = self.minotaurInitialPosition\n",
    "        self.playerMovDict = generateMoveDict(mapGrid, ignore_walls=False)\n",
    "        self.minotaurMovDict = generateMoveDict(mapGrid)\n",
    "        self.stateGrid = generateStateGrid(mapGrid)\n",
    "        self.state = self.__mapPosToState(self.playerPos)\n",
    "        self.steps = 0\n",
    "        self.done = False\n",
    "        \n",
    "    def reset(self):\n",
    "        self.steps = 0\n",
    "        self.done = False\n",
    "        self.playerPos = self.playerInitialPosition\n",
    "        self.minotaurPos = self.minotaurInitialPosition\n",
    "        self.state = self.__mapPosToState(self.playerPos)\n",
    "    \n",
    "    def __mapPosToState(self, coordinate):\n",
    "        posVariable = coordinate[0]*8 + coordinate[1]\n",
    "        minotaurVariable = self.minotaurPos[0]*8 + self.minotaurPos[1]\n",
    "        \n",
    "        return minotaurVariable*7*8 + posVariable\n",
    "    \n",
    "    def __nextPlayerPosition(self, action):\n",
    "        return self.playerMovDict[self.playerPos[0]][self.playerPos[1]][action]\n",
    "    \n",
    "    def __nextMinotaurPosition(self):\n",
    "        return self.minotaurMovDict[self.minotaurPos[0]][self.minotaurPos[1]][np.random.choice(4, 1)[0]]\n",
    "    \n",
    "    def step(self, action):\n",
    "        if (self.done):\n",
    "            self.done = True\n",
    "            return self.state, 0, True\n",
    "        newPlayerPosition = self.__nextPlayerPosition(action)\n",
    "        \n",
    "        if (newPlayerPosition == self.minotaurPos):\n",
    "            # Same cell as minotaur\n",
    "            self.done = True\n",
    "            self.state = self.__mapPosToState(newPlayerPosition)\n",
    "            self.steps =+ 1\n",
    "            return self.state, -20, True\n",
    "        \n",
    "        if (newPlayerPosition == self.minotaurInitialPosition):\n",
    "            # Arrive to the exit\n",
    "            self.done = True\n",
    "            self.steps =+ 1\n",
    "            self.state = self.__mapPosToState(newPlayerPosition)\n",
    "            return self.state, self.T + 5, True\n",
    "        \n",
    "        self.minotaurPos = self.__nextMinotaurPosition()\n",
    "        \n",
    "        if (self.minotaurPos == newPlayerPosition):\n",
    "            # Same cell as minotaur after minotaur moved\n",
    "            self.done = True\n",
    "            self.state = self.__mapPosToState(newPlayerPosition)\n",
    "            self.steps =+ 1\n",
    "            return self.state, -20, True\n",
    "        \n",
    "        self.steps =+ 1\n",
    "        \n",
    "        if (self.steps == self.T):\n",
    "            self.done = True\n",
    "            self.state = self.__mapPosToState(newPlayerPosition)\n",
    "            return self.state, -self.T, True\n",
    "        \n",
    "        if (newPlayerPosition == self.playerPos):\n",
    "            # Same position as before\n",
    "            self.state = self.__mapPosToState(newPlayerPosition)\n",
    "            return self.state, -5, False\n",
    "        \n",
    "        self.playerPos = newPlayerPosition\n",
    "        self.state = self.__mapPosToState(newPlayerPosition)\n",
    "        return self.state, -1, False\n",
    "\n",
    "def getGridFromState(state):\n",
    "    grid = deepcopy(mapGrid)\n",
    "    \n",
    "    playerState = state % (7*8)\n",
    "    playerCoord = (math.floor(playerState/8), playerState % 8)\n",
    "    \n",
    "    minotaurState = math.floor(state/(7*8))\n",
    "    minotaurCoord = (math.floor(minotaurState/8), minotaurState % 8)\n",
    "    \n",
    "    grid[playerCoord[0]][playerCoord[1]] = 42\n",
    "    grid[minotaurCoord[0]][minotaurCoord[1]] = 99\n",
    "    \n",
    "    return grid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "\n",
    "    def __init__(self, nA=6, epsilon=1.0, alpha=.05, gamma=1.0):\n",
    "        \"\"\" Initialize agent.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        - nA: number of actions available to the agent\n",
    "        - epsilon: parameter for epsilon-Greedy algorithm\n",
    "        - alpha: \"Learning rate\"\n",
    "        - gamma: Discount factor\n",
    "        \"\"\"\n",
    "        self.nA = nA\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "\n",
    "    def __update_Q(self, current_Q, next_Q, reward):\n",
    "        \"\"\" Updates the action-value function estimate using the most recent time step \"\"\"\n",
    "        return current_Q + (self.alpha * (reward + (self.gamma * next_Q) - current_Q))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\" Given the state, select an action \"\"\"\n",
    "        policy_probabilities = np.ones(self.nA) * self.epsilon / self.nA\n",
    "        policy_probabilities[np.argmax(self.Q[state])] = 1 - self.epsilon + (self.epsilon / self.nA)\n",
    "\n",
    "        return np.random.choice(np.arange(self.nA), p=policy_probabilities)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, episode, done):\n",
    "        \"\"\" Update the agent's knowledge, using the most recently sampled tuple \"\"\"\n",
    "        self.epsilon = 1.0 / episode\n",
    "        \n",
    "        self.Q[state][action] = self.__update_Q(self.Q[state][action], np.max(self.Q[next_state]), reward)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact(env, agent, num_episodes=20000, window=100):\n",
    "    \"\"\" Monitor agents performance\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "    - env: instance of OpenAI Gym's Taxi-v1 environment\n",
    "    - agent: instance of class Agent (see Agent.py for details)\n",
    "    - num_episodes: number of episodes of agent-environment interaction\n",
    "    - window: number of episodes to consider when calculating average rewards\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    - avg_rewards: deque containing average rewards\n",
    "    - best_avg_reward: largest value in the avg_rewards deque\n",
    "    \"\"\"\n",
    "    avg_rewards = deque(maxlen=num_episodes)\n",
    "    best_avg_reward = -math.inf\n",
    "    samp_rewards = deque(maxlen=window)\n",
    "    convergence_episode = 0\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        state = env.reset()\n",
    "        samp_reward = 0\n",
    "        \n",
    "        # Steps in episode\n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, i_episode, done)\n",
    "            \n",
    "            # update the sampled reward\n",
    "            samp_reward += reward\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                # save final sampled reward\n",
    "                samp_rewards.append(samp_reward)\n",
    "                break\n",
    "        if (i_episode >= window):\n",
    "            # get average reward from last 100 episodes\n",
    "            avg_reward = np.mean(samp_rewards)\n",
    "            avg_rewards.append(avg_reward)\n",
    "\n",
    "            if avg_reward > best_avg_reward:\n",
    "                convergence_episode = i_episode\n",
    "                best_avg_reward = avg_reward\n",
    "\n",
    "        print(\"\\rEpisode {}/{} || Best average reward {}\".format(i_episode, num_episodes, best_avg_reward), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        if i_episode == num_episodes: print('\\n')\n",
    "    return avg_rewards, best_avg_reward, convergence_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20000/20000 || Best average reward 15.668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = QLearningAgent(nA=4, gamma=.9, alpha=.04, epsilon=0.9)\n",
    "env = MinotaurEnvironment(T=30)\n",
    "avg_rewards, best_avg_reward, episode_best_reward = interact(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "env.reset()\n",
    "steps = 0\n",
    "states = []\n",
    "# Steps in episode\n",
    "while True:\n",
    "    steps = steps + 1\n",
    "    action = agent.select_action(state)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    # update the sampled reward\n",
    "    state = next_state\n",
    "    states.append(getGridFromState(state))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  1  1  1  1 99  1  0]\n",
      " [ 0  0  0  0  1  0  0  0]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [42  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  1  1  1  1  1 99  0]\n",
      " [ 0  0  0  0  1  0  0  0]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [42  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  1  1  1  1  1  1  0]\n",
      " [ 0  0  0  0  1  0 99  0]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [42  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  1  1  1  1  1  1  0]\n",
      " [ 0  0  0  0  1  0 99  0]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0 42  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  1  1  1  1  1 99  0]\n",
      " [ 0  0  0  0  1  0  0  0]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0 42  0  0  0  0  0  0]\n",
      " [ 0  1  1  1  1  1  1  0]\n",
      " [ 0  0  0  0  1  0 99  0]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0 42  0  0  0  0  0]\n",
      " [ 0  1  1  1  1  1  1  0]\n",
      " [ 0  0  0  0  1 99  0  0]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0 42  0  0  0  0]\n",
      " [ 0  1  1  1  1  1  1  0]\n",
      " [ 0  0  0  0  1 99  0  0]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0 42  0  0  0]\n",
      " [ 0  1  1  1  1 99  1  0]\n",
      " [ 0  0  0  0  1  0  0  0]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0 42  0  0  0  0]\n",
      " [ 0  1  1  1  1  1 99  0]\n",
      " [ 0  0  0  0  1  0  0  0]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0 42  0  0  0]\n",
      " [ 0  1  1  1  1  1  1  0]\n",
      " [ 0  0  0  0  1  0 99  0]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0 42  0  0]\n",
      " [ 0  1  1  1  1  1  1  0]\n",
      " [ 0  0  0  0  1  0  0 99]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0 42  0]\n",
      " [ 0  1  1  1  1  1  1  0]\n",
      " [ 0  0  0  0  1  0  0 99]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0 42]\n",
      " [ 0  1  1  1  1  1  1  0]\n",
      " [ 0  0  0  0  1  0 99  0]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  1  1  1  1  1 99 42]\n",
      " [ 0  0  0  0  1  0  0  0]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  1  1  1  1  1  1 99]\n",
      " [ 0  0  0  0  1  0  0 42]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  1  1  1  1  1  1 99]\n",
      " [ 0  0  0  0  1  0 42  0]]\n",
      "[[ 0  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  1  1  1]\n",
      " [ 0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  1  1  1  1  1  1 99]\n",
      " [ 0  0  0  0  1 42  0  0]]\n"
     ]
    }
   ],
   "source": [
    "def print_states(states):\n",
    "    for state in states:\n",
    "        print(\"{}\\n\".format(state), end=\"\")\n",
    "        time.sleep(1)\n",
    "        sys.stdout.flush()        \n",
    "print_states(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
